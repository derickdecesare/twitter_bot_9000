{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_arxiv(query, max_results=1):\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    search_query = query.replace(' ', '+')\n",
    "    url = f\"{base_url}search_query=all:{search_query}&start=0&max_results={max_results}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def download_arxiv_pdf(arxiv_id, save_path):\n",
    "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return f\"PDF downloaded and saved to {save_path}\"\n",
    "    else:\n",
    "        return f\"Failed to download PDF. Status code: {response.status_code}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Result (raw XML):\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3Adeep%20rl%20in%20llms%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=all:deep rl in llms&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
      "  <id>http://arxiv.org/api/VKkYz4lI6LafGlPn0ufclG7hHs4</id>\n",
      "  <updated>2024-08-02T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">2367834</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.01874v1</id>\n",
      "    <updated>2024-02-02T20:01:15Z</updated>\n",
      "    <published>2024-02-02T20:01:15Z</published>\n",
      "    <title>The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement\n",
      "  Learning and Large Language Models</title>\n",
      "    <summary>  In this work, we review research studies that combine Reinforcement Learning\n",
      "(RL) and Large Language Models (LLMs), two areas that owe their momentum to the\n",
      "development of deep neural networks. We propose a novel taxonomy of three main\n",
      "classes based on the way that the two model types interact with each other. The\n",
      "first class, RL4LLM, includes studies where RL is leveraged to improve the\n",
      "performance of LLMs on tasks related to Natural Language Processing. L4LLM is\n",
      "divided into two sub-categories depending on whether RL is used to directly\n",
      "fine-tune an existing LLM or to improve the prompt of the LLM. In the second\n",
      "class, LLM4RL, an LLM assists the training of an RL model that performs a task\n",
      "that is not inherently related to natural language. We further break down\n",
      "LLM4RL based on the component of the RL training framework that the LLM assists\n",
      "or replaces, namely reward shaping, goal generation, and policy function.\n",
      "Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a\n",
      "common planning framework without either of them contributing to training or\n",
      "fine-tuning of the other. We further branch this class to distinguish between\n",
      "studies with and without natural language feedback. We use this taxonomy to\n",
      "explore the motivations behind the synergy of LLMs and RL and explain the\n",
      "reasons for its success, while pinpointing potential shortcomings and areas\n",
      "where further research is needed, as well as alternative methodologies that\n",
      "serve the same goal.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Moschoula Pternea</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Prerna Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abir Chakraborty</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yagna Oruganti</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mirco Milletari</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sayli Bapat</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kebei Jiang</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">30 pages (including bibliography), 1 figure, 7 tables</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.01874v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.01874v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual test\n",
    "query = \"deep rl in llms\"\n",
    "search_result = search_arxiv(query)\n",
    "print(\"Search Result (raw XML):\")\n",
    "print(search_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded and saved to downloaded_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# For simplicity, let's assume the first result is what we want\n",
    "# In a real scenario, you'd parse the XML to extract the ID properly\n",
    "#  <id>http://arxiv.org/api/VKkYz4lI6LafGlPn0ufclG7hHs4</id>\n",
    "arxiv_id = \"2402.01874v1\"\n",
    "\n",
    "download_result = download_arxiv_pdf(arxiv_id, \"downloaded_paper.pdf\")\n",
    "print(download_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THERL/LLM T AXONOMY TREE: REVIEWING SYNERGIES\n",
      "BETWEEN REINFORCEMENT LEARNING AND LARGE LANGUAGE\n",
      "MODELS\n",
      "Moschoula Pternea\n",
      "Microsoft\n",
      "mpternea@microsoft.comPrerna Singh\n",
      "Microsoft\n",
      "prernasingh@microsoft.comAbir Chakraborty\n",
      "Microsoft\n",
      "abir.chakraborty@microsoft.com\n",
      "Yagna Oruganti\n",
      "Microsoft\n",
      "yaorugan@microsoft.comMirco Milletari\n",
      "Microsoft\n",
      "mimillet@microsoft.comSayli Bapat\n",
      "Microsoft\n",
      "saylibapat@microsoft.com\n",
      "Kebei Jiang\n",
      "Microsoft\n",
      "kebei.jiang@microsoft.com\n",
      "ABSTRACT\n",
      "In this work, we review research studies that combine Reinforcement Learning (RL) and Large\n",
      "Language Models (LLMs), two areas that owe their momentum to the development of deep neural\n",
      "networks. We propose a novel taxonomy of three main classes based on the way that the two model\n",
      "types interact with each other. The first class, RL4LLM , includes studies where RL is leveraged to\n",
      "improve the performance of LLMs on tasks related to Natural Language Processing. RL4LLM is\n",
      "divided into two sub-categories depending on whether RL is used to directly fine-tune an existing\n",
      "LLM or to improve the prompt of the LLM. In the second class, LLM4RL , an LLM assists the training\n",
      "of an RL model that performs a task that is not inherently related to natural language. We further\n",
      "break down LLM4RL based on the component of the RL training framework that the LLM assists\n",
      "or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class,\n",
      "RL+LLM , an LLM and an RL agent are embedded in a common planning framework witho\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Usage example\n",
    "pdf_path = \"downloaded_paper.pdf\"  # Make sure this matches the path where you saved the PDF\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "print(extracted_text[:1500])  # Print the first 500 characters as a sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import PyPDF2\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "anthropic_api_key = os.environ['ANTHROPIC_API_KEY']\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", api_key=anthropic_api_key)\n",
    "\n",
    "@tool\n",
    "def search_arxiv(query: str, max_results: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Searches arXiv for papers based on the given query.\n",
    "    Returns a list of paper titles and their arXiv IDs.\n",
    "    \"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    search_query = query.replace(' ', '+')\n",
    "    url = f\"{base_url}search_query=all:{search_query}&start=0&max_results={max_results}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error: Unable to fetch results. Status code: {response.status_code}\"\n",
    "    \n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    results = []\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
    "        arxiv_id = entry.find('{http://www.w3.org/2005/Atom}id').text.split('/')[-1]\n",
    "        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()\n",
    "        results.append(f\"Title: {title}\\nArXiv ID: {arxiv_id}\\nSummary: {summary}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(results) if results else \"No results found.\"\n",
    "\n",
    "# @tool\n",
    "# def choose_paper(arxiv_id: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Selects a paper by its arXiv ID.\n",
    "#     \"\"\"\n",
    "#     return arxiv_id\n",
    "\n",
    "@tool\n",
    "def download_and_extract_text(arxiv_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads the PDF for a given arXiv ID and extracts its text content.\n",
    "    \"\"\"\n",
    "    url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return f\"Failed to download PDF. Status code: {response.status_code}\"\n",
    "    \n",
    "    with open(\"temp.pdf\", 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    text = \"\"\n",
    "    with open(\"temp.pdf\", 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    \n",
    "    os.remove(\"temp.pdf\")  # Clean up the temporary file\n",
    "    \n",
    "    return text[:1000000]  # Limit to first 1,000,000 characters to avoid potential token limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='white-space: pre-wrap; word-wrap: break-word;'>[HumanMessage(content='Search arXiv for 3 papers related to: Find papers about deep rl in llms and reasoning.'),\n",
       " AIMessage(content=[{'text': \"Certainly! I'll use the search_arxiv function to find 3 papers related to deep reinforcement learning (RL) in large language models (LLMs) and reasoning. Here's the function call:\", 'type': 'text'}, {'id': 'toolu_01MLHT5MkxocMxGEbBcTnbom', 'input': {'query': 'deep reinforcement learning language models reasoning', 'max_results': 3}, 'name': 'search_arxiv', 'type': 'tool_use'}], response_metadata={'id': 'msg_01TEgC1tXao4vGRmVWsVfGeA', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 495, 'output_tokens': 125}}, id='run-fed42f40-bf94-4e52-a3a9-420dce940846-0', tool_calls=[{'name': 'search_arxiv', 'args': {'query': 'deep reinforcement learning language models reasoning', 'max_results': 3}, 'id': 'toolu_01MLHT5MkxocMxGEbBcTnbom', 'type': 'tool_call'}], usage_metadata={'input_tokens': 495, 'output_tokens': 125, 'total_tokens': 620}),\n",
       " ToolMessage(content=\"Title: Enhancing Logical Reasoning in Large Language Models to Facilitate Legal\\n  Applications\\nArXiv ID: 2311.13095v1\\nSummary: Language serves as a vehicle for conveying thought, enabling communication\\namong individuals. The ability to distinguish between diverse concepts,\\nidentify fairness and injustice, and comprehend a range of legal notions\\nfundamentally relies on logical reasoning. Large Language Models (LLMs) attempt\\nto emulate human language understanding and generation, but their competency in\\nlogical reasoning remains limited. This paper seeks to address the\\nphilosophical question: How can we effectively teach logical reasoning to LLMs\\nwhile maintaining a deep understanding of the intricate relationship between\\nlanguage and logic? By focusing on bolstering LLMs' capabilities in logical\\nreasoning, we aim to expand their applicability in law and other\\nlogic-intensive disciplines. To this end, we propose a Reinforcement Learning\\nfrom Logical Feedback (RLLF) approach, which serves as a potential framework\\nfor refining LLMs' reasoning capacities. Through RLLF and a revised evaluation\\nmethodology, we explore new avenues for research in this domain and contribute\\nto the development of LLMs capable of handling complex legal reasoning tasks\\nwhile acknowledging the fundamental connection between language and logic.\\n\\nTitle: Influencing Reinforcement Learning through Natural Language Guidance\\nArXiv ID: 2104.01506v2\\nSummary: Interactive reinforcement learning agents use human feedback or instruction\\nto help them learn in complex environments. Often, this feedback comes in the\\nform of a discrete signal that is either positive or negative. While\\ninformative, this information can be difficult to generalize on its own. In\\nthis work, we explore how natural language advice can be used to provide a\\nricher feedback signal to a reinforcement learning agent by extending policy\\nshaping, a well-known Interactive reinforcement learning technique. Usually\\npolicy shaping employs a human feedback policy to help an agent to learn more\\nabout how to achieve its goal. In our case, we replace this human feedback\\npolicy with policy generated based on natural language advice. We aim to\\ninspect if the generated natural language reasoning provides support to a deep\\nreinforcement learning agent to decide its actions successfully in any given\\nenvironment. So, we design our model with three networks: first one is the\\nexperience driven, next is the advice generator and third one is the advice\\ndriven. While the experience driven reinforcement learning agent chooses its\\nactions being influenced by the environmental reward, the advice driven neural\\nnetwork with generated feedback by the advice generator for any new state\\nselects its actions to assist the reinforcement learning agent to better policy\\nshaping.\\n\\nTitle: Weakly Supervised Reasoning by Neuro-Symbolic Approaches\\nArXiv ID: 2309.13072v1\\nSummary: Deep learning has largely improved the performance of various natural\\nlanguage processing (NLP) tasks. However, most deep learning models are\\nblack-box machinery, and lack explicit interpretation. In this chapter, we will\\nintroduce our recent progress on neuro-symbolic approaches to NLP, which\\ncombines different schools of AI, namely, symbolism and connectionism.\\nGenerally, we will design a neural system with symbolic latent structures for\\nan NLP task, and apply reinforcement learning or its relaxation to perform\\nweakly supervised reasoning in the downstream task. Our framework has been\\nsuccessfully applied to various tasks, including table query reasoning,\\nsyntactic structure reasoning, information extraction reasoning, and rule\\nreasoning. For each application, we will introduce the background, our\\napproach, and experimental results.\\n\", name='search_arxiv', tool_call_id='toolu_01MLHT5MkxocMxGEbBcTnbom')]</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pprint\n",
    "\n",
    "tools = [search_arxiv, download_and_extract_text]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Initial query\n",
    "query = \"Find papers about deep rl in llms and reasoning.\"\n",
    "messages = [HumanMessage(content=f\"Search arXiv for 3 papers related to: {query}\")]\n",
    "\n",
    "# Prompt the LLM to search arXiv\n",
    "search_response = llm_with_tools.invoke(messages)\n",
    "# print(search_response.content)\n",
    "# print(search_response.tool_calls)\n",
    "messages.append(search_response)\n",
    "\n",
    "# Execute the tool call to actually perform the search\n",
    "search_results = None\n",
    "for tool_call in search_response.tool_calls:\n",
    "    if tool_call[\"name\"].lower() == \"search_arxiv\":\n",
    "        search_results_tool_msg = search_arxiv.invoke(tool_call)\n",
    "        break\n",
    "\n",
    "if search_results_tool_msg is None:\n",
    "    raise ValueError(\"The LLM did not call the search_arxiv tool as expected.\")\n",
    "\n",
    "# Add the search results to the messages\n",
    "messages.append(search_results_tool_msg)\n",
    "\n",
    "\n",
    "\n",
    "def format_messages(messages):\n",
    "    formatted_output = \"<pre style='white-space: pre-wrap; word-wrap: break-word;'>\"\n",
    "    formatted_output += pprint.pformat(messages, width=80, depth=None)\n",
    "    formatted_output += \"</pre>\"\n",
    "    return HTML(formatted_output)\n",
    "\n",
    "# At the end of your script, replace print(messages) with:\n",
    "display(format_messages(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Based on the search results, the most relevant and interesting paper appears to be:\\n\\n\"Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications\" (ArXiv ID: 2311.13095v1)\\n\\nThis paper seems the most relevant because:\\n\\n1. It directly addresses the topic of logical reasoning in Large Language Models (LLMs), which is closely related to your interest in deep reinforcement learning (RL) in LLMs and reasoning.\\n\\n2. It proposes a novel approach called Reinforcement Learning from Logical Feedback (RLLF), which combines reinforcement learning with logical reasoning in LLMs.\\n\\n3. The paper aims to improve LLMs\\' capabilities in complex reasoning tasks, particularly in the legal domain, which demonstrates a practical application of the research.\\n\\nLet\\'s use the download_and_extract_text tool to get more information about this paper:', 'type': 'text'}, {'id': 'toolu_01KnarnJ6SP3ZQgx4WiNgSPs', 'input': {'arxiv_id': '2311.13095v1'}, 'name': 'download_and_extract_text', 'type': 'tool_use'}]\n",
      "[{'name': 'download_and_extract_text', 'args': {'arxiv_id': '2311.13095v1'}, 'id': 'toolu_01KnarnJ6SP3ZQgx4WiNgSPs', 'type': 'tool_call'}]\n",
      "Enhancing Logical Reasoning in Large\n",
      "Language Models to Facilitate Legal\n",
      "Applications\n",
      "Ha-Thanh NguyenaWachara FungwacharakornaKen Satoha\n",
      "aNational Institute of Informatics, Japan\n",
      "Abstract. Language serves as a vehicle for conveying thought, enabling commu-\n",
      "nication among individuals. The ability to distinguish between diverse concepts,\n",
      "identify fairness and injustice, and comprehend a range of legal notions fundamen-\n",
      "tally relies on logical reasoning. Large Language Models (LLMs) attempt to emu-\n",
      "late human language understanding and generation, but their competency in logical\n",
      "reasoning remains limited. This paper seeks to address the philosophical question:\n",
      "How can we effectively teach logical reasoning to LLMs while maintaining a deep\n",
      "understanding of the intricate relationship between language and logic? By focus-\n",
      "ing on bolstering LLMsâ€™ capabilities in logical reasoning, we aim to expand their\n",
      "applicability in law and other logic-intensive disciplines. To this end, we propose\n",
      "a Rein\n",
      "Certainly! I'll provide a summary of the paper and then create a Twitter thread highlighting its key points and significance.\n",
      "\n",
      "Summary:\n",
      "The paper \"Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications\" addresses the limitations of Large Language Models (LLMs) in logical reasoning, particularly in the legal domain. The authors propose a novel approach called Reinforcement Learning from Logical Feedback (RLLF) to improve LLMs' logical reasoning capabilities while minimizing human biases. This approach combines both human feedback and logical feedback to train the reward predictor, aiming to enhance LLMs' performance in complex legal reasoning tasks. The paper also explores various logical representations for feedback and discusses their potential impact on capturing LLMs' logical reasoning abilities.\n",
      "\n",
      "Now, let's create a Twitter thread highlighting the key points and significance of this paper:\n",
      "\n",
      "1/7 New research alert! ðŸš¨ \"Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications\" proposes a novel approach to improve LLMs' reasoning capabilities in the legal domain. #AI #LegalTech #MachineLearning\n",
      "\n",
      "2/7 The paper introduces Reinforcement Learning from Logical Feedback (RLLF), an innovative method that combines human feedback with logical reasoning engines to train LLMs. This approach aims to reduce human biases and enhance logical reasoning. #RLLF #AIEthics\n",
      "\n",
      "3/7 Key challenge: LLMs like GPT-3 and GPT-4 excel in many language tasks but struggle with complex logical reasoning required in legal applications. RLLF addresses this limitation by incorporating logical feedback in the training process. #LLMs #LogicalReasoning\n",
      "\n",
      "4/7 RLLF framework:\n",
      "1. Generate responses\n",
      "2. Train reward predictor using human & logical feedback\n",
      "3. Retrain LLM using RL algorithms\n",
      "This balanced approach optimizes for both user satisfaction and logical accuracy. #ReinforcementLearning #AI\n",
      "\n",
      "5/7 The paper explores various logical representations for feedback, considering factors like popularity, complexity, and consistency. This analysis helps determine the most effective ways to capture and improve LLMs' logical reasoning. #LogicProgramming #AIResearch\n",
      "\n",
      "6/7 Significance: RLLF has the potential to greatly improve LLMs' performance in logic-intensive domains like law, enhancing their ability to provide accurate, logically sound solutions while maintaining user satisfaction. #LegalAI #FutureOfLaw\n",
      "\n",
      "7/7 This research opens new avenues for developing more capable and reliable AI systems for legal applications, potentially revolutionizing how legal professionals interact with and utilize AI technologies. Exciting times ahead! ðŸš€ #AIInnovation #LegalInnovation\n"
     ]
    }
   ],
   "source": [
    "# Ask LLM to choose the most relevant paper\n",
    "messages.append(HumanMessage(content=\"Based on these results, which paper seems most relevant and interesting? Please use the download_and_extract_text tool to indicate your choice by providing the arXiv ID.\"))\n",
    "paper_choice = llm_with_tools.invoke(messages)\n",
    "print(paper_choice.content)\n",
    "print(paper_choice.tool_calls)\n",
    "messages.append(paper_choice)\n",
    "\n",
    "# Extract chosen arXiv ID from the tool call and invoke the download_and_extract_text tool\n",
    "pdf_text_response = None\n",
    "for tool_call in paper_choice.tool_calls:\n",
    "    if tool_call['name'].lower() == 'download_and_extract_text':\n",
    "        pdf_text_response = download_and_extract_text.invoke(tool_call)\n",
    "        break\n",
    "\n",
    "if pdf_text_response is None:\n",
    "    raise ValueError(\"The LLM did not make a valid paper choice using the tool.\")\n",
    "\n",
    "# add it to the messages\n",
    "messages.append(pdf_text_response)\n",
    "print(pdf_text_response.content[:1000])\n",
    "\n",
    "\n",
    "# Ask LLM to summarize the paper and create a Twitter thread\n",
    "messages.append(HumanMessage(content=\"Based on the extracted text, please provide a summary of the paper and create a Twitter thread (5-7 tweets) highlighting its key points and significance.\"))\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "print(final_response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
